{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Garbage Classifier Performance Metrics**\n",
        "### **ENEL 645**\n",
        "#### **Group 5: Destin Saba, Cole Cathcart**\n",
        "\n",
        "Introuction..."
      ],
      "metadata": {
        "id": "O1zkeyNiF-AH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-w0yL3h7F58B",
        "outputId": "897b0d14-6df3-4c09-c33f-2050a9b64094",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models, datasets\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision.datasets import ImageFolder\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from torchvision.models import ResNet18_Weights\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sja8pdNaF58D"
      },
      "outputs": [],
      "source": [
        "# Set path for test data and best model depending on where the notebook is\n",
        "# being run\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "TEST_PATH = \"/content/drive/MyDrive/CVPR_2024_dataset_Test\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/best_model.pth\"\n",
        "\n",
        "# TEST_PATH = \"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Test\"\n",
        "# TEST_PATH = \"/path/to/local/dataset\"\n",
        "# MODEL_PATH = \"./best_model.pth\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to redefine some classes from the python file in order to load and preprocess the test set for our best model:"
      ],
      "metadata": {
        "id": "lMdO2W0sPrfK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwGGJAFrF58D"
      },
      "outputs": [],
      "source": [
        "# Transformations to the test set\n",
        "transform = {\n",
        "    \"test\": transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Tokenizer for text\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Class to preprocess dataset\n",
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, tokenizer=None):\n",
        "        self.dataset = datasets.ImageFolder(root=root_dir, transform=transform)\n",
        "        self.tokenizer = tokenizer  # Store tokenizer reference\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.dataset.samples[idx]\n",
        "\n",
        "        # Load and transform image\n",
        "        image = self.dataset.loader(img_path)\n",
        "        if self.dataset.transform:\n",
        "            image = self.dataset.transform(image)\n",
        "\n",
        "        # Extract filename text and tokenize\n",
        "        filename = os.path.splitext(os.path.basename(img_path))[0]  # Safer file parsing\n",
        "        filename = filename.replace('_', ' ')\n",
        "        text_inputs = self.tokenizer(filename, padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"pt\")\n",
        "\n",
        "        input_ids = text_inputs[\"input_ids\"]\n",
        "        attention_mask = text_inputs[\"attention_mask\"]\n",
        "        if input_ids.dim() > 1:\n",
        "            input_ids = input_ids.squeeze(0)\n",
        "        if attention_mask.dim() > 1:\n",
        "            attention_mask = attention_mask.squeeze(0)\n",
        "\n",
        "        return image, input_ids, attention_mask, label\n",
        "\n",
        "# Image + text classifier\n",
        "class ImageTextClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_rate=0.0):\n",
        "        super(ImageTextClassifier, self).__init__()\n",
        "\n",
        "        # Image feature extractor\n",
        "        self.image_extractor = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Freeze the weights of the image extractor\n",
        "        for param in self.image_extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Remove the final layer of the image extractor\n",
        "        self.image_extractor.fc = nn.Identity()\n",
        "\n",
        "        # Text feature extractor\n",
        "        self.text_extractor = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # Freeze first 4 layers of the text extractor, unfreeze last 2 for fine-tuning\n",
        "        for i, param in enumerate(self.text_extractor.parameters()):\n",
        "            if i < len(list(self.text_extractor.parameters())) - 2:\n",
        "                param.requires_grad = False\n",
        "            else:\n",
        "                param.requires_grad = True\n",
        "\n",
        "        # Reduce text feature dimensionality\n",
        "        self.text_fc = nn.Linear(self.text_extractor.config.hidden_size, 256)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Classifier (image output size is 512, text output size is 256)\n",
        "        self.classifier = nn.Linear(512 + 256, num_classes)\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        # Extract image features\n",
        "        image_features = self.image_extractor(images)\n",
        "\n",
        "        # Extract text features\n",
        "        text_outputs = self.text_extractor(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Reduce text feature dimensionality\n",
        "        text_features = text_outputs.last_hidden_state[:, 0, :]\n",
        "        text_features = self.text_fc(text_features)\n",
        "\n",
        "        # Concatenate image and text features\n",
        "        features = torch.cat((image_features, text_features), dim=1)\n",
        "\n",
        "        # Apply dropout\n",
        "        features = self.dropout(features)\n",
        "\n",
        "        # Classify\n",
        "        output = self.classifier(features)\n",
        "\n",
        "        return output\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, input_ids, attention_mask, labels in dataloader:\n",
        "            images, input_ids, attention_mask, labels = (\n",
        "                images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            )\n",
        "\n",
        "            outputs = model(images, input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_loss = running_loss / len(dataloader.dataset)\n",
        "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we use our functions to preprocess the test set:"
      ],
      "metadata": {
        "id": "wN1e21wZUkj-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amRArp1bF58D",
        "outputId": "4dc6175d-d322-4e40-f19d-83b53b2ab524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Black', 'Blue', 'Green', 'Other']\n",
            "Train set: 32\n",
            "Val set: 16\n",
            "Test set: 16\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load datasets\n",
        "datasets = {\n",
        "    \"test\": ImageTextDataset(TEST_PATH, transform=transform[\"test\"], tokenizer=tokenizer),\n",
        "}\n",
        "\n",
        "print(f\"Dataset sizes - Train: {len(datasets['train'])}, Val: {len(datasets['val'])}, Test: {len(datasets['test'])}\")\n",
        "\n",
        "# Create dataloaders\n",
        "dataloaders = {\n",
        "    \"test\": DataLoader(datasets[\"test\"], batch_size=32, shuffle=False),\n",
        "}\n",
        "\n",
        "# Load the best model from file\n",
        "best_model = ImageTextClassifier(num_classes=4)\n",
        "best_model.load_state_dict(torch.load(MODEL_PATH, map_location=device, weights_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_metrics = evaluate_model(best_model, dataloaders[\"test\"], nn.CrossEntropyLoss(), device)\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, input_ids, attention_mask, labels in dataloaders[\"test\"]:\n",
        "        images, input_ids, attention_mask, labels = (\n",
        "            images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        )\n",
        "        outputs = best_model(images, input_ids, attention_mask)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())"
      ],
      "metadata": {
        "id": "yiJmxfIaX9U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can print the evaluation metrics and visualize the model's performance to aid in analysis:"
      ],
      "metadata": {
        "id": "9VW3l5fXYZeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set metrics\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "print(f\"Loss: {test_metrics['loss']:.4f}\")\n",
        "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
        "print(f\"F1 Score: {test_metrics['f1']:.4f}\")"
      ],
      "metadata": {
        "id": "oSSSEb4fZIDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per-class metrics\n",
        "class_names = datasets['test'].dataset.classes\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"{class_name}:\")\n",
        "    print(f\"  Precision: {precision[i]:.4f}\")\n",
        "    print(f\"  Recall: {recall[i]:.4f}\")\n",
        "    print(f\"  F1 Score: {f1[i]:.4f}\")\n",
        "    print(f\"  Support: {support[i]}\")"
      ],
      "metadata": {
        "id": "dG4hfkKFZsgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zrW8Mk6ubIv4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}